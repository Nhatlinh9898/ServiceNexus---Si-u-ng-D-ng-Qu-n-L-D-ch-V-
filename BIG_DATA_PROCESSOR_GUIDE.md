# Big Data Processor Guide

## üéØ **T·ªïng Quan**

Big Data Processor l√† h·ªá th·ªëng AI chuy√™n s√¢u ƒë·ªÉ x·ª≠ l√Ω h√†ng t·ª∑ t·ª∑ file v·ªõi kh·∫£ nƒÉng:
- **Massive Data Processing**: X·ª≠ l√Ω d·ªØ li·ªáu quy m√¥ l·ªõn v·ªõi chunking v√† parallel processing
- **Data Link Resolution**: T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu li√™n k·∫øt v·ªõi infinite loop prevention
- **Infinite Loop Prevention**: NgƒÉn ch·∫∑n v√≤ng l·∫∑p v√¥ h·∫°n th√¥ng qua pattern detection
- **Memory Management**: Qu·∫£n l√Ω b·ªô nh·ªõ hi·ªáu qu·∫£ v·ªõi worker threads
- **Fault Tolerance**: X·ª≠ l√Ω l·ªói v√† recovery mechanisms

## üöÄ **T√≠nh NƒÉng Ch√≠nh**

### **1. Big Data Processing**
- **Chunk-based Processing**: Chia d·ªØ li·ªáu th√†nh chunks ƒë·ªÉ x·ª≠ l√Ω song song
- **Multi-threaded Processing**: S·ª≠ d·ª•ng worker threads ƒë·ªÉ t·ªëi ∆∞u performance
- **Memory Optimization**: Qu·∫£n l√Ω b·ªô nh·ªõ v·ªõi streaming v√† garbage collection
- **Format Support**: H·ªó tr·ª£ JSON, CSV, XML, TXT, JSONL formats
- **Data Validation**: Validate v√† clean data t·ª± ƒë·ªông
- **Deduplication**: Lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p
- **Compression**: N√©n d·ªØ li·ªáu ƒë·ªÉ ti·∫øt ki·ªám storage

### **2. Data Link Resolution**
- **Recursive Crawling**: ƒê·ªá quy t·∫£i d·ªØ li·ªáu t·ª´ nhi·ªÅu URLs
- **Infinite Loop Prevention**: Ph√°t hi·ªán v√† ngƒÉn ch·∫∑n v√≤ng l·∫∑p v√¥ h·∫°n
- **Pattern Detection**: Ph√¢n t√≠ch URL patterns ƒë·ªÉ detect suspicious behavior
- **Domain Filtering**: L·ªçc domains cho ph√©p v√† ch·∫∑n
- **File Type Filtering**: Ch·ªâ t·∫£i c√°c file types ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
- **Size Limiting**: Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc file ƒë·ªÉ tr√°nh memory overflow
- **Rate Limiting**: Ki·ªÉm so√°t t·ªëc ƒë·ªô t·∫£i ƒë·ªÉ tr√°nh b·ªã block

### **3. Circuit Breaker Pattern**
- **Failure Detection**: T·ª± ƒë·ªông detect khi service kh√¥ng available
- **Circuit States**: CLOSED, OPEN, HALF_OPEN states
- **Automatic Recovery**: T·ª± ƒë·ªông ph·ª•c h·ªìi khi service available
- **Fallback Mechanisms**: Alternative processing paths

### **4. Advanced Features**
- **Progress Tracking**: Real-time progress monitoring
- **Error Handling**: Comprehensive error logging v√† recovery
- **Performance Metrics**: Detailed performance analytics
- **Resource Monitoring**: CPU, memory, disk usage tracking
- **Scalability**: Horizontal scaling support

## üìÅ **C·∫•u Tr√∫c H·ªá Th·ªëng**

```
services/
‚îú‚îÄ‚îÄ bigDataProcessor.js      # Core big data processing engine
‚îî‚îÄ‚îÄ dataLinkResolver.js      # Data link resolution engine

server/routes/
‚îî‚îÄ‚îÄ big-data.js              # API endpoints for big data processing

src/components/
‚îî‚îÄ‚îÄ BigDataProcessor.tsx     # Frontend management interface

data/bigdata/                 # Local storage
‚îú‚îÄ‚îÄ chunks/                  # Processed data chunks
‚îú‚îÄ‚îÄ downloads/               # Downloaded files
‚îú‚îÄ‚îÄ processed/               # Final processed data
‚îî‚îÄ‚îÄ errors/                  # Error logs
```

## üîß **API Endpoints**

### **Big Data Processing**
- `POST /api/big-data/process` - Process big data file
- `POST /api/big-data/download-and-process` - Download v√† process URL
- `POST /api/big-data/batch-process` - Batch process multiple files
- `POST /api/big-data/validate-file` - Validate file tr∆∞·ªõc processing
- `POST /api/big-data/create-plan` - Create processing plan
- `GET /api/big-data/big-data/stats` - Big data processing statistics

### **Data Link Resolution**
- `POST /api/big-data/resolve-links` - Resolve data links
- `GET /api/big-data/links/stats` - Link resolution statistics
- `GET /api/big-data/links/suspicious-patterns` - Suspicious patterns
- `GET /api/big-data/links/failed-urls` - Failed URLs list

### **System Management**
- `GET /api/big-data/status` - System status
- `GET /api/big-data/health` - Health check
- `GET /api/big-data/queue-status` - Queue status
- `POST /api/big-data/cleanup` - Cleanup resources
- `GET /api/big-data/export/:filename` - Export processed data

## üíª **Frontend Interface**

### **Main Features**
- **2 Main Tabs**: Big Data Processing, Link Resolution
- **Configuration Panel**: Advanced configuration options
- **Real-time Monitoring**: Live progress tracking
- **Statistics Dashboard**: Comprehensive metrics display
- **Error Handling**: User-friendly error messages

### **User Interface**
- **Processing Configuration**: Chunk size, workers, formats, etc.
- **Link Resolution Setup**: Seed URLs, depth limits, filters
- **Progress Visualization**: Real-time progress bars v√† charts
- **System Health**: Circuit breaker status, resource usage
- **Advanced Options**: Fine-tuning parameters

## üéØ **S·ª≠ D·ª•ng**

### **1. Process Big Data File**
```javascript
// Frontend
const processBigData = async (config) => {
  const response = await fetch('/api/big-data/process', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      source: 'https://example.com/large-dataset.json',
      format: 'json',
      chunkSize: 10000,
      maxWorkers: 4,
      outputFormat: 'json',
      compression: true,
      validation: true,
      deduplication: true
    })
  });
  
  const result = await response.json();
  console.log('Processing started:', result.data);
};

// Monitor progress
const monitorProgress = async () => {
  const response = await fetch('/api/big-data/big-data/stats');
  const stats = await response.json();
  console.log('Progress:', stats.data);
};
```

### **2. Resolve Data Links**
```javascript
// Frontend
const resolveDataLinks = async (config) => {
  const response = await fetch('/api/big-data/resolve-links', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      seedUrls: [
        'https://example.com/data.json',
        'https://example.org/more-data.csv'
      ],
      maxDepth: 3,
      maxConcurrentDownloads: 5,
      allowedDomains: ['example.com', 'example.org'],
      fileTypes: ['json', 'csv', 'txt'],
      maxFileSize: 104857600, // 100MB
      delayBetweenRequests: 1000
    })
  });
  
  const result = await response.json();
  console.log('Link resolution started:', result.data);
};
```

### **3. Download and Process URL**
```javascript
// Frontend
const downloadAndProcess = async (url) => {
  const response = await fetch('/api/big-data/download-and-process', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      source: url,
      format: 'json',
      chunkSize: 5000,
      maxWorkers: 2,
      outputFormat: 'json',
      compression: true,
      maxRetries: 3,
      timeout: 30000
    })
  });
  
  const result = await response.json();
  console.log('Download and process result:', result.data);
};
```

### **4. Batch Processing**
```javascript
// Frontend
const batchProcess = async (files) => {
  const response = await fetch('/api/big-data/batch-process', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      files: [
        {
          source: 'https://example.com/data1.json',
          format: 'json',
          chunkSize: 10000
        },
        {
          source: 'https://example.com/data2.csv',
          format: 'csv',
          chunkSize: 5000
        }
      ],
      commonOptions: {
        compression: true,
        validation: true,
        deduplication: true
      }
    })
  });
  
  const result = await response.json();
  console.log('Batch processing result:', result.data);
};
```

### **5. Backend Usage**
```javascript
const BigDataProcessor = require('./services/bigDataProcessor');
const DataLinkResolver = require('./services/dataLinkResolver');

// Initialize processors
const bigDataProcessor = new BigDataProcessor();
const dataLinkResolver = new DataLinkResolver();

await bigDataProcessor.initialize();
await dataLinkResolver.initialize();

// Process big data
const result = await bigDataProcessor.processBigData({
  source: 'https://example.com/large-dataset.json',
  format: 'json',
  chunkSize: 10000,
  maxWorkers: 4,
  compression: true,
  validation: true,
  deduplication: true
});

// Resolve data links
const linkResult = await dataLinkResolver.resolveDataLinks([
  'https://example.com/data.json'
], {
  maxDepth: 3,
  maxConcurrentDownloads: 5,
  fileTypes: ['json', 'csv'],
  maxFileSize: 104857600
});
```

## üìä **Data Formats**

### **Input Formats**
```json
// JSON/JSONL
{"id": 1, "name": "John", "age": 30}
{"id": 2, "name": "Jane", "age": 25}

// CSV
id,name,age
1,John,30
2,Jane,25

// XML
<records>
  <record id="1" name="John" age="30" />
  <record id="2" name="Jane" age="25" />
</records>

// TXT
Record 1: John, 30
Record 2: Jane, 25
```

### **Output Formats**
```json
// Processed JSON
{
  "metadata": {
    "source": "https://example.com/data.json",
    "processedAt": "2024-01-01T00:00:00Z",
    "recordCount": 1000000
  },
  "data": [
    {"id": 1, "name": "John", "age": 30, "processed": true},
    {"id": 2, "name": "Jane", "age": 25, "processed": true}
  ]
}
```

## üîí **Infinite Loop Prevention**

### **Pattern Detection Algorithm**
```javascript
// URL Pattern Creation
const createUrlPattern = (url) => {
  const urlObj = new URL(url);
  return `${urlObj.protocol}//${urlObj.host}${urlObj.pathname}`;
};

// Loop Detection
const detectInfiniteLoop = (url, parentUrl) => {
  const urlPattern = createUrlPattern(url);
  const parentPattern = createUrlPattern(parentUrl);
  
  // Track pattern history
  if (!patternHistory.has(urlPattern)) {
    patternHistory.set(urlPattern, []);
  }
  
  const history = patternHistory.get(urlPattern);
  history.push({ url, parentUrl, timestamp: Date.now() });
  
  // Check for repeating patterns
  if (history.length > maxRepeats) {
    const recentPatterns = history.slice(-maxRepeats);
    const uniqueParents = new Set(recentPatterns.map(p => p.parentUrl));
    
    if (uniqueParents.size <= 2) {
      return true; // Infinite loop detected
    }
  }
  
  return false;
};
```

### **Circular Reference Detection**
```javascript
const hasCircularReference = (urlChain) => {
  const seen = new Set();
  for (const url of urlChain) {
    if (seen.has(url)) {
      return true; // Circular reference detected
    }
    seen.add(url);
  }
  return false;
};
```

### **Suspicious Pattern Detection**
```javascript
const suspiciousPatterns = [
  /\/\d+\/\d+\/\d+/,  // Deep numeric paths
  /\?page=\d+&page=\d+/,  // Duplicate pagination
  /\/repeat\/repeat\/repeat/,  // Repeating segments
  /\?id=\d+&id=\d+/  // Duplicate parameters
];

const isSuspiciousPattern = (url) => {
  return suspiciousPatterns.some(pattern => pattern.test(url));
};
```

## üõ†Ô∏è **Configuration**

### **Environment Variables**
```bash
# Big Data Processing
BIG_DATA_CHUNK_SIZE=10000
BIG_DATA_MAX_WORKERS=4
BIG_DATA_MAX_FILE_SIZE=104857600
BIG_DATA_COMPRESSION=true
BIG_DATA_VALIDATION=true

# Link Resolution
LINK_MAX_DEPTH=3
LINK_MAX_CONCURRENT_DOWNLOADS=5
LINK_MAX_FILE_SIZE=104857600
LINK_DELAY_BETWEEN_REQUESTS=1000
LINK_RESPECT_ROBOTS_TXT=true

# Circuit Breaker
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RESET_TIMEOUT=60000
CIRCUIT_BREAKER_HALF_OPEN_MAX_CALLS=3
```

### **Advanced Configuration**
```javascript
const bigDataProcessor = new BigDataProcessor({
  chunkSize: 10000,
  maxConcurrentWorkers: 4,
  maxFileSize: 100 * 1024 * 1024, // 100MB
  compression: true,
  validation: true,
  deduplication: true,
  circuitBreaker: {
    failureThreshold: 5,
    resetTimeout: 60000,
    halfOpenMaxCalls: 3
  }
});

const dataLinkResolver = new DataLinkResolver({
  maxDepth: 3,
  maxConcurrentDownloads: 5,
  allowedDomains: ['example.com', 'trusted-site.org'],
  blockedDomains: ['spam-site.com'],
  fileTypes: ['json', 'csv', 'txt'],
  maxFileSize: 100 * 1024 * 1024,
  delayBetweenRequests: 1000,
  infiniteLoopDetector: {
    maxRepeats: 3,
    suspiciousPatterns: new Set()
  }
});
```

## üö® **Error Handling**

### **Common Errors**
- **File Too Large**: File v∆∞·ª£t gi·ªõi h·∫°n k√≠ch th∆∞·ªõc
- **Memory Overflow**: V∆∞·ª£t gi·ªõi h·∫°n b·ªô nh·ªõ
- **Network Timeout**: Request timeout
- **Circuit Breaker Open**: Service kh√¥ng available
- **Infinite Loop Detected**: Ph√°t hi·ªán v√≤ng l·∫∑p v√¥ h·∫°n
- **Invalid Format**: File format kh√¥ng h·ª£p l·ªá

### **Error Recovery**
```javascript
// Retry Mechanism
const retryOperation = async (operation, maxRetries = 3) => {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      if (attempt === maxRetries) throw error;
      
      const delay = Math.pow(2, attempt) * 1000; // Exponential backoff
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
};

// Circuit Breaker Pattern
const executeWithCircuitBreaker = async (operation) => {
  if (circuitBreaker.state === 'OPEN') {
    throw new Error('Circuit breaker is OPEN');
  }
  
  try {
    const result = await operation();
    circuitBreaker.reset();
    return result;
  } catch (error) {
    circuitBreaker.recordFailure();
    throw error;
  }
};
```

## üìà **Performance Optimization**

### **Memory Management**
- **Streaming Processing**: X·ª≠ l√Ω d·ªØ li·ªáu theo lu·ªìng
- **Garbage Collection**: D·ªçn d·∫πp b·ªô nh·ªõ t·ª± ƒë·ªông
- **Object Pooling**: T√°i s·ª≠ d·ª•ng objects
- **Memory Monitoring**: Theo d√µi s·ª≠ d·ª•ng b·ªô nh·ªõ

### **Processing Optimization**
- **Parallel Processing**: X·ª≠ l√Ω song song chunks
- **Chunk Size Tuning**: T·ªëi ∆∞u k√≠ch th∆∞·ªõc chunk
- **Worker Thread Pool**: Qu·∫£n l√Ω worker threads hi·ªáu qu·∫£
- **Load Balancing**: Ph√¢n ph·ªëi workload

### **Network Optimization**
- **Connection Pooling**: T√°i s·ª≠ d·ª•ng connections
- **Rate Limiting**: Ki·ªÉm so√°t t·ªëc ƒë·ªô request
- **Request Batching**: Gom nh√≥m requests
- **Compression**: N√©n d·ªØ li·ªáu truy·ªÅn t·∫£i

## üîß **Troubleshooting**

### **Performance Issues**
1. **Slow Processing**: TƒÉng s·ªë l∆∞·ª£ng workers, gi·∫£m chunk size
2. **Memory Leaks**: Monitor memory usage, restart workers
3. **Network Timeouts**: TƒÉng timeout values, retry mechanism
4. **Infinite Loops**: Check pattern detection, adjust thresholds

### **Data Issues**
1. **Invalid Format**: Validate input format, use robust parsers
2. **Large Files**: Increase chunk size, use streaming
3. **Corrupted Data**: Implement data validation and recovery
4. **Encoding Issues**: Handle different character encodings

### **System Issues**
1. **Disk Space**: Monitor disk usage, implement cleanup
2. **CPU Usage**: Optimize algorithms, use efficient data structures
3. **Network Issues**: Implement retry logic, circuit breaker
4. **Resource Exhaustion**: Monitor resources, implement limits

## üìö **Examples**

### **Process Large JSON Dataset**
```javascript
const processLargeDataset = async () => {
  const processor = new BigDataProcessor();
  await processor.initialize();
  
  const result = await processor.processBigData({
    source: 'https://example.com/large-dataset.json',
    format: 'json',
    chunkSize: 50000, // 50k records per chunk
    maxWorkers: 8, // 8 worker threads
    outputFormat: 'json',
    compression: true,
    validation: true,
    deduplication: true,
    dataTransformation: (record) => ({
      ...record,
      processed_at: new Date().toISOString(),
      normalized_name: record.name.toLowerCase().trim()
    })
  });
  
  console.log(`Processed ${result.recordCount} records`);
  console.log(`Output saved to: ${result.outputPath}`);
};
```

### **Crawl Data from Multiple Sources**
```javascript
const crawlMultipleSources = async () => {
  const resolver = new DataLinkResolver();
  await resolver.initialize();
  
  const result = await resolver.resolveDataLinks([
    'https://api.example.com/users',
    'https://data.example.org/products',
    'https://files.example.net/reports'
  ], {
    maxDepth: 2,
    maxConcurrentDownloads: 3,
    allowedDomains: ['example.com', 'example.org', 'example.net'],
    fileTypes: ['json', 'csv'],
    maxFileSize: 50 * 1024 * 1024, // 50MB
    delayBetweenRequests: 2000,
    followRedirects: true,
    respectRobotsTxt: true
  });
  
  console.log(`Downloaded ${result.length} files`);
  console.log(`Total size: ${formatFileSize(result.reduce((sum, r) => sum + r.downloadResult.size, 0))}`);
};
```

### **Batch Processing with Error Handling**
```javascript
const batchProcessWithErrorHandling = async (files) => {
  const processor = new BigDataProcessor();
  await processor.initialize();
  
  const results = [];
  const errors = [];
  
  for (const file of files) {
    try {
      const result = await processor.processBigData({
        source: file.url,
        format: file.format,
        chunkSize: 10000,
        maxWorkers: 2,
        outputFormat: 'json',
        compression: true,
        validation: true,
        deduplication: true
      });
      
      results.push(result);
    } catch (error) {
      errors.push({
        file: file.url,
        error: error.message,
        timestamp: new Date().toISOString()
      });
    }
  }
  
  console.log(`Successfully processed: ${results.length} files`);
  console.log(`Failed: ${errors.length} files`);
  
  return { results, errors };
};
```

## üéØ **Best Practices**

### **Data Processing**
- **Start Small**: Test v·ªõi small datasets tr∆∞·ªõc khi scale up
- **Monitor Resources**: Theo d√µi CPU, memory, disk usage
- **Validate Input**: Validate data format v√† structure
- **Handle Errors Gracefully**: Implement robust error handling
- **Use Streaming**: Process large files theo lu·ªìng

### **Link Resolution**
- **Respect Robots.txt**: Tu√¢n th·ªß robots.txt rules
- **Rate Limiting**: Kh√¥ng overload target servers
- **Domain Filtering**: Ch·ªâ crawl trusted domains
- **Size Limits**: Set reasonable file size limits
- **Loop Prevention**: Implement strong loop detection

### **System Design**
- **Scalability**: Design cho horizontal scaling
- **Reliability**: Implement fault tolerance
- **Monitoring**: Comprehensive logging v√† metrics
- **Security**: Validate inputs, sanitize outputs
- **Performance**: Optimize cho large datasets

---

## üéØ **K·∫øt Lu·∫≠n**

Big Data Processor cung c·∫•p gi·∫£i ph√°p to√†n di·ªán cho vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu quy m√¥ l·ªõn v·ªõi:
- **Massive Data Processing**: X·ª≠ l√Ω h√†ng t·ª∑ t·ª∑ file hi·ªáu qu·∫£
- **Infinite Loop Prevention**: NgƒÉn ch·∫∑n v√≤ng l·∫∑p v√¥ h·∫°n th√¥ng minh
- **Data Link Resolution**: T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu li√™n k·∫øt t·ª± ƒë·ªông
- **Fault Tolerance**: X·ª≠ l√Ω l·ªói v√† recovery mechanisms
- **Performance Optimization**: T·ªëi ∆∞u h√≥a cho large-scale processing

H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu c·ª±c l·ªõn m·ªôt c√°ch an to√†n, hi·ªáu qu·∫£ v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông.
